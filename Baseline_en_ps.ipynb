{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_en_ps.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZztj7JkI0HFHFWcGLB5Fr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"L8oK1k3C8IyV","colab_type":"code","outputId":"6857ae9d-4160-4b47-d05f-04c492b1d714","executionInfo":{"status":"ok","timestamp":1587276338349,"user_tz":240,"elapsed":19398,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8BM71zNk9C1-","colab_type":"code","colab":{}},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","  \n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","import torch\n","import torch.nn as nn \n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from io import open\n","from collections import Counter\n","from functools import partial\n","import unicodedata\n","import re\n","from torch.autograd import Variable\n","from gensim.models import KeyedVectors\n","from gensim.models.wrappers import FastText\n","import random\n","import time\n","from datetime import datetime\n","import pickle as pkl\n","import string\n","import os\n","from os import listdir \n","from ast import literal_eval\n","from nltk.tokenize import WordPunctTokenizer \n","\n","\n","import numpy as np \n","import pandas as pd \n","import torch\n","import torch.nn as nn \n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import random\n","import math \n","\n","\n","\n","\n","\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","import torch\n","import torch.nn as nn \n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import sacrebleu\n","import random\n","import time\n","from datetime import datetime\n","import pickle as pkl\n","import string\n","import os\n","from os import listdir \n","from ast import literal_eval\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.style\n","import matplotlib as mpl\n","from collections import OrderedDict\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jev6MwG3KM6d","colab_type":"code","colab":{}},"source":["#!pip install sacrebleu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJHiRKo3cNwh","colab_type":"code","colab":{}},"source":["SRC_LANG = 'en'\n","TARG_LANG = 'ps'\n","\n","SRC_MAX_SENTENCE_LEN = 10\n","TARG_MAX_SENTENCE_LEN = 10\n","SRC_VOCAB_SIZE = 10000 \n","TARG_VOCAB_SIZE = 10000 \n","\n","BATCH_SIZE = 64"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZIceAKsv8NM","colab_type":"text"},"source":["## Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"wVZdXJTg4_Jm","colab_type":"text"},"source":["### Split and save raw data into train.src.tok, train.targ.tok, val.src.tok, val.targ.tok, test.src.tok, test.targ.tok"]},{"cell_type":"code","metadata":{"id":"52PTi2UR3yYl","colab_type":"code","colab":{}},"source":["#### Create full data for (English, Pashto): combining 4 files each (not needed if you have the dataset in your directory)\n","\n","# en_ps_EN = ['bible.en-ps.clean.en', 'KDE4.en-ps.en', 'ted-wmt20.en-ps.en', 'Ubuntu.en-ps.en']\n","# en_ps_PS = ['bible.en-ps.clean.ps', 'KDE4.en-ps.ps', 'ted-wmt20.en-ps.ps', 'Ubuntu.en-ps.ps']\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/en.tok', 'w') as outfile:\n","#     for file_name in en_ps_EN:\n","#         with open(\"/content/drive/My Drive/ds1012/MT/data/en-ps/{}\".format(file_name)) as infile:\n","#             outfile.write(infile.read())\n","#         outfile.write('\\n')\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/ps.tok', 'w') as outfile:\n","#     for file_name in en_ps_PS:\n","#         with open(\"/content/drive/My Drive/ds1012/MT/data/en-ps/{}\".format(file_name)) as infile:\n","#             outfile.write(infile.read())\n","#         outfile.write('\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFLDa2sR3DUW","colab_type":"code","colab":{}},"source":["#### Create train, dev, test for (English, Pashto)\n","\n","# random.seed(1234)\n","\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/en.tok') as infile:\n","#     data = infile.readlines()\n","# train_idx = int(len(data)*0.7)\n","# dev_idx = int(len(data)*0.85)\n","# random.shuffle(data)\n","# train = data[:train_idx]\n","# dev = data[train_idx:dev_idx]\n","# test = data[dev_idx:]\n","\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/train.en.tok', 'w') as outfile:\n","#     outfile.write(\"\".join(train))\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/dev.en.tok', 'w') as outfile:\n","#     outfile.write(\"\".join(dev))\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/test.en.tok', 'w') as outfile:\n","#     outfile.write(\"\".join(test))\n","\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/ps.tok') as infile:\n","#     data = infile.readlines()\n","\n","# random.shuffle(data)\n","# train = data[:train_idx]\n","# dev = data[train_idx:dev_idx]\n","# test = data[dev_idx:]\n","\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/train.ps.tok', 'w') as outfile:\n","#     outfile.write(\"\".join(train))\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/dev.ps.tok', 'w') as outfile:\n","#     outfile.write(\"\".join(dev))\n","# with open('/content/drive/My Drive/ds1012/MT/data/en-ps/test.ps.tok', 'w') as outfile:\n","#     outfile.write(\"\".join(test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzyxoc3874r0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"outputId":"d2e1837f-c558-41aa-807b-fe9be7a05ac8","executionInfo":{"status":"ok","timestamp":1587245294555,"user_tz":240,"elapsed":332,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["# show examples for ps\n","with open('/content/drive/My Drive/ds1012/MT/data/en-ps/train.ps.tok') as f:\n","    data = f.read().split('\\n')\n","data[:10]\n","# Note: data are not clean where both en and ps has sentence like '%d:%02d:%02d'"],"execution_count":155,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['مخبره',\n"," 'دا د اِسمٰعيل زامن وُو ، چې په دې ترتيب سره پېدا شوى وُو : نبايوت ، قيدار ، ادبيئل ، مِبسام ،',\n"," 'د کډي د کتابتونونو لپاره په مختاړي کې وژباړل شو',\n"," 'د قوم د راغونډولو د خبر د پاره به دواړه بيګلې غږولے شى . خو دا آواز به لنډ نۀ وى .',\n"," 'په پرليکه اکر کې پېلول',\n"," '. دا خوښبکس وتوانوﺉ چې د ليکبڼه ډول امستنې بدلې کړﺉ@ info: tooltip',\n"," 'عيسىٰ دَ هغوئ سره دَ غرۀ نه راکُوز شو اَؤ په هوار ميدان کښے ودريدو . په دغه ځائے کښے دَ هغۀ ګڼ مُريدان اَؤ ډير خلق چه ټول دَ يهُوديه اَؤ بيتُ المُقدس دَ صور اَؤ دَ صيدا دَ سمندرى غاړے نه راټول شوى وُو اَؤ دَ هغۀ آؤريدو ته راغلى وُو اَؤ چه دوئ دَ خپلو رنځُونو نه شفا ومُومى .',\n"," 'ننوت لېلې_',\n"," 'خو کله چه هغوئ دَ عيسىٰ خوا ته راغلل نو هغه ئے وليدو چه مړ دے ، نو هغوئ دَ هغۀ پښے ماتے نۀ کړلے .',\n"," '%d:%02d:%02d5:02:%Id%dshort time format']"]},"metadata":{"tags":[]},"execution_count":155}]},{"cell_type":"code","metadata":{"id":"TF-rm5DGGtZK","colab_type":"code","colab":{}},"source":["#### Create train, dev, test for (English, Odia)\n","\n","# for i in ['train', 'dev', 'test']:\n","#     with open('/content/drive/My Drive/ds1012/MT/data/en-od/{}.final'.format(i)) as infile:\n","#         lines = infile.readlines()\n","#     en_od_EN = []\n","#     en_od_OD = []\n","#     for line in lines:\n","#         en_od_EN.append(line.split('\\t')[1])\n","#         en_od_OD.append(line.split('\\t')[2])\n","#     with open('/content/drive/My Drive/ds1012/MT/data/en-od/{}.en.tok'.format(i), 'w') as outfile:\n","#         outfile.write(\"\\n\".join(en_od_EN))\n","#     with open('/content/drive/My Drive/ds1012/MT/data/en-od/{}.od.tok'.format(i), 'w') as outfile:\n","#         outfile.write(\"\".join(en_od_OD))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cAQpuj4Imsz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"82b75876-7017-4554-e99e-37e25758168d","executionInfo":{"status":"ok","timestamp":1587245250477,"user_tz":240,"elapsed":261,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["#### Show examples for od\n","with open('/content/drive/My Drive/ds1012/MT/data/en-od/train.od.tok') as f:\n","    data = f.read().split('\\n')\n","data[:10]"],"execution_count":154,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ଆରମ୍ଭରେ ପରମେଶ୍ବର ଆକାଶ ଓ ପୃଥିବୀକୁ ସୃଷ୍ଟି କଲେ।',\n"," 'ପୃଥିବୀ ସେତବେେଳେ ସଂପୂରନ୍ଭାବେ ଶୂନ୍ଯ ଓ କିଛି ନଥିଲା। ଜଳଭାଗ ଉପରେ ଅନ୍ଧକାର ଘାଡ଼ଇେେ ରଖିଥିଲା ଏବଂ ପରମେଶ୍ବରଙ୍କର ଆତ୍ମା ଜଳଭାଗ ଉପରେ ବ୍ଯାପ୍ତ ଥିଲା।',\n"," 'ପରମେଶ୍ବର ଆଲୋକକୁ ଦେଖିଲେ ଏବଂ ସେ ଜାଣିଲେ, ତାହା ଉତ୍ତମ, ଏହାପ ରେ ପରମେଶ୍ବର ଆଲୋକକୁ ଅନ୍ଧକାରରୁ ଅଲଗା କଲେ।',\n"," 'ପରମେଶ୍ବର ସହେି ଆଲୋକର ନାମ ଦେଲେ \" ଦିନ\" ଏବଂ ଅନ୍ଧକାରର ନାମ ଦେଲେ \"ରାତି।\"',\n"," 'ଏହାପରେ ପରମେଶ୍ବର କହିଲେ, \"ଜଳ ମଧିଅରେ ବୃହତ ଗମ୍ବୁଜ ଜାତ ହାଇେ ଜଳକୁ ଦୁଇଭାଗ କରୁ!\"',\n"," 'ଏହିପରି ପରମେଶ୍ବର ତାରଣେ ନିର୍ମାଣ କଲେ ଏବଂ ତାଣେ ଉପର ଜଳଠାରୁ ତାରଣେ ତଳ ଜଳକୁ ଅଲଗା କଲେ। ତହିଁରେ ସହେିପରି ହେଲା।',\n"," 'ପରମେଶ୍ବର ସହେି ତାରଣେ ନାମ ଦେଲେ \"ଆକାଶ\" ତା\\'ପରେ ସଠାେ ରେ ପ୍ରଭାତ ଏବଂ ସଠାେରେ ସୁର୍ୟ୍ଯାସ୍ତ ହେଲା। ଏବଂ ଏହା ଦି୍ବତୀଯ ଦିନ ଥିଲା।',\n"," 'ଏହାପରେ ପରମେଶ୍ବର କହିଲେ, \"ଆକାଶମଣ୍ଡଳ ଅଧଃସ୍ଥ ସମଗ୍ର ଜଳ ଏକ ସ୍ଥାନ ରେ ସଂଗୃହିତ ହେଉ। ୟଦ୍ବାରା ଭୂମି ଶୁଖିଲା ଦଖାୟିବେ।\" ଏବଂ ଏହିପରି ହେଲା।',\n"," 'ପରମେଶ୍ବର ଶୁଖିଲା ଭୂମିର ନାମ \"ପୃଥିବୀ\" ଦେଲେ। ଏବଂ ଜଳସମୁହ ଭାଗର ନାମ ଦେଲେ, \"ସମୁଦ୍ର।\" ଏହା ପରମେଶ୍ବରଙ୍କ ଦୃଷ୍ଟିରେ ଅତି ଉତ୍ତମ ଦିଶିଥିଲା।',\n"," 'ପରମେଶ୍ବର କହିଲେ, \"ପୃଥିବୀ ତୃଣ ଓ ସଜୀବ ଶାକ, ଜୀବ ସମ୍ବଳିତ ନିଜ ନିଜ ଜାତି ଅନୁୟାଯୀ ଫଳ ଉତ୍ପନ୍ନ କରୁ,\" ଏହିପରି ହେଲା।']"]},"metadata":{"tags":[]},"execution_count":154}]},{"cell_type":"markdown","metadata":{"id":"IoUv2m-7wGVd","colab_type":"text"},"source":["### Generate Vocab and Tokenize"]},{"cell_type":"code","metadata":{"id":"z8hlbmXHyF5b","colab_type":"code","colab":{}},"source":["RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n","\n","def get_filepath(split, src_lang, targ_lang, lang_type): \n","    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n","        and the language type (source or target) \n","        e.g. to load train.en.tok for en-ps pair, use get_filepath(split='train', src_lang='en', targ_lang='ps', lang_type='source')\n","    \"\"\"\n","    folder_name = \"/content/drive/My Drive/ds1012/MT/data/{}-{}/\".format(src_lang, targ_lang)\n","    if lang_type == 'source': \n","        file_name = \"{}.{}.tok\".format(split, src_lang)\n","    elif lang_type == 'target': \n","        file_name = \"{}.{}.tok\".format(split, targ_lang)\n","    return folder_name + file_name\n","\n","\n","def build_vocab(token_lists, max_vocab_size): \n","    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size and returns: \n","        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n","        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n","        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) most frequently occuring tokens\n","    \"\"\"\n","    num_vocab = max_vocab_size - len(RESERVED_TOKENS)\n","    all_tokens = [token for sublist in token_lists for token in sublist]\n","    token_counter = Counter(all_tokens)\n","    vocab, count = zip(*token_counter.most_common(num_vocab))\n","    id2token = sorted(RESERVED_TOKENS, key=RESERVED_TOKENS.get) + list(vocab)\n","    token2id = dict(zip(id2token, range(max_vocab_size)))\n","    \n","    # check how many unique tokens + pct of corpus are represented in our vocab \n","    tokens_in_vocab_pct_corpus = 100 * sum([token_counter[token] for token in vocab]) / len(all_tokens)\n","    print(\"A vocabulary of {} is generated from a set of {} unique tokens, representing {:.1f}% of entire corpus\".format(\n","        len(vocab), len(token_counter), tokens_in_vocab_pct_corpus))\n","    \n","    return token2id, id2token \n","\n","\n","\n","def generate_vocab(src_lang, targ_lang, src_vocab_size, targ_vocab_size):\n","    \"\"\" \n","        Takes source and target language names and vocab sizes, outputs a nested dictionary vocab \n","        containing token2id and id2token for both source and target languages. \n","        Note the first level of keys is lang_name (e.g. 'en'), and that of nested dictionary are token2id and id2token.\n","    \"\"\"\n","    vocab = {} \n","    for lang, vocab_size in zip([src_lang, targ_lang], [src_vocab_size, targ_vocab_size]): \n","        \n","        # load train data \n","        train_data_fp = get_filepath(split='train', src_lang=src_lang, targ_lang=targ_lang, \n","                                     lang_type='source' if lang == 'en' else 'target')\n","        # tokenize train data\n","        tk = WordPunctTokenizer() \n","        with open(train_data_fp) as f:\n","            train_tokens = [tk.tokenize(line) for line in f.readlines()]\n","\n","        # generate token2id and id2token \n","        token2id, id2token = build_vocab(train_tokens, vocab_size) \n","        \n","        # store token2id, id2token as a dict in nested dict lang \n","        vocab[lang] = {'token2id': token2id, 'id2token': id2token}\n","        \n","    return vocab"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrYvZioLPL6O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"51975d54-8e59-4146-9fd6-755adae57297","executionInfo":{"status":"ok","timestamp":1587247823126,"user_tz":240,"elapsed":984,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["### it takes a long time to generate vocabulary, so save to pickle for reimport in future \n","\n","## en-ps pair\n","# SRC_LANG = 'en'\n","# TARG_LANG = 'ps'\n","# SRC_VOCAB_SIZE = 10000 \n","# TARG_VOCAB_SIZE = 10000 \n","# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n","# vocab_filename = \"/content/drive/My Drive/ds1012/MT/vocab/{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n","# pkl.dump(vocab, open(vocab_filename, \"wb\"))"],"execution_count":180,"outputs":[{"output_type":"stream","text":["A vocabulary of 9996 is generated from a set of 14064 unique tokens, representing 98.6% of entire corpus\n","A vocabulary of 9996 is generated from a set of 16814 unique tokens, representing 98.3% of entire corpus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zj9r73yNXLYi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"dee91bd4-b5d6-4716-fbfa-7fa74c7713b3","executionInfo":{"status":"ok","timestamp":1587248197081,"user_tz":240,"elapsed":2026,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["# # en-ps pair\n","# SRC_LANG = 'en'\n","# TARG_LANG = 'od'\n","# SRC_VOCAB_SIZE = 10000 \n","# TARG_VOCAB_SIZE = 10000 \n","# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n","# vocab_filename = \"/content/drive/My Drive/ds1012/MT/vocab/{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n","# pkl.dump(vocab, open(vocab_filename, \"wb\"))"],"execution_count":188,"outputs":[{"output_type":"stream","text":["A vocabulary of 9996 is generated from a set of 13069 unique tokens, representing 99.6% of entire corpus\n","A vocabulary of 6242 is generated from a set of 6242 unique tokens, representing 100.0% of entire corpus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5O5Cvi57WFy_","colab_type":"code","colab":{}},"source":["# reload from pickle for en-ps\n","SRC_LANG = 'en'\n","TARG_LANG = 'ps'\n","SRC_VOCAB_SIZE = 10000 \n","TARG_VOCAB_SIZE = 10000\n","\n","vocab_filename = \"/content/drive/My Drive/ds1012/MT/vocab/{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n","vocab = pkl.load(open(vocab_filename, \"rb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VaLv-3KPWFdU","colab_type":"code","colab":{}},"source":["# usage \n","## vocab['en']['id2token']\n","## vocab['ps']['token2id']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZNb6UqV-cLx7","colab_type":"text"},"source":["### Generate Data using Vocab"]},{"cell_type":"code","metadata":{"id":"G66g-ayLXxWu","colab_type":"code","colab":{}},"source":["def get_filepaths(src_lang, targ_lang): \n","    \"\"\" Takes language names ('ps', 'en') to be translated from and to (in_lang and out_lang respectively) as inputs, \n","        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n","        e.g. fps['train']['source']['filepath']\n","    \"\"\"\n","    fps = {} \n","    \n","    # store language names \n","    fps['languages'] = {} \n","    fps['languages']['source'] = src_lang\n","    fps['languages']['target'] = targ_lang \n","    \n","    # store filepaths \n","    for split in ['train', 'dev', 'test']: \n","        fps[split] = {} \n","        for lang_type in ['source', 'target']: \n","            fps[split][lang_type] = {} \n","            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n","            \n","    return fps\n","\n","\n","def text2tokens(raw_text_fp, lang_type): \n","    \"\"\" Takes filepath of raw text and outputs a list of lists, each representing a sentence of words (tokens) \n","        Note that it appends to target sentences <SOS> at the start, and <EOS> at the end, but only <EOS> at the end for source sentences\n","    \"\"\"\n","    with open(raw_text_fp) as f:\n","        tk = WordPunctTokenizer()\n","        tokens_data = [tk.tokenize(line) for line in f.readlines()]\n","        if lang_type == 'source': \n","            tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n","        elif lang_type == 'target': \n","            tokens_data = [['<SOS>'] + datum + ['<EOS>'] for datum in tokens_data]\n","    return tokens_data \n","\n","\n","def tokens2indices(tokens_data, token2id): \n","    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n","    indices_data = [] \n","    for datum in tokens_data: \n","        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n","        indices_data.append(indices_datum)    \n","    return indices_data\n","\n","\n","def process_data(src_lang, targ_lang, src_max_sentence_len, targ_max_sentence_len, vocab, sample_limit=None, filter_long=True): \n","    \"\"\" \n","        - Main function that takes source and target language names, vocab dict generated, \n","        and an optional sample_limit representing the number of sentences to subset if necessary (for evaluation).\n","        we filter out long senstences whose length goes above src(targ)_max_sentence_len if filter_long\n","        - Returns data as a nested dictionary containing the indices and tokens of train/dev/test data \n","        for both source and target languages. \n","        - Note the hierachy of data dict is: data[split][lang_type]['tokens' or 'indices'], \n","        e.g. to access indices of source training data, use data['train']['source']['indices'] or data['train']['source']['tokens']\n","    \"\"\" \n","    \n","    # get filepaths \n","    data = get_filepaths(src_lang, targ_lang)\n","    \n","    # loop through each file, read in text, convert to tokens, then to indices \n","    for split in ['train', 'dev', 'test']: \n","        for lang_type in ['source', 'target']: \n","            # read in tokens \n","            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'], lang_type)\n","    \n","    # for training data, keep only pairs with both source and target sentences within max_sent_len \n","    if filter_long: \n","        original_train_size = len(data['train']['source']['tokens'])\n","        source_lengths = np.array([len(l) for l in data['train']['source']['tokens']])\n","        target_lengths = np.array([len(l) for l in data['train']['target']['tokens']])\n","        keep_mask = (source_lengths <= src_max_sentence_len) & (target_lengths <= targ_max_sentence_len)\n","        data['train']['source']['tokens'] = list(np.array(data['train']['source']['tokens'])[keep_mask])\n","        data['train']['target']['tokens'] = list(np.array(data['train']['target']['tokens'])[keep_mask])\n","        new_train_size = len(data['train']['source']['tokens']) \n","        print(\"{} data points are removed from training data after filtering out long sentences: {} remain.\".format(\n","            new_train_size - original_train_size, new_train_size))\n","    # further limit number of samples if applicable \n","    if sample_limit is not None: \n","        for split in ['train', 'dev', 'test']: \n","            for lang_type in ['source', 'target']: \n","                data[split][lang_type]['tokens'] = data[split][lang_type]['tokens'][:sample_limit]\n","\n","    # convert tokens to indices \n","    for split in ['train', 'dev', 'test']: \n","        for lang_type in ['source', 'target']: \n","            data[split][lang_type]['indices'] = tokens2indices(tokens_data=data[split][lang_type]['tokens'],  \n","                token2id = vocab[data['languages'][lang_type]]['token2id'])\n","\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zE2ptrCoXxQC","colab_type":"code","colab":{}},"source":["# Load data for en-ps\n","data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n","data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n","data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PooErl-qXw7w","colab_type":"text"},"source":["### Create Dataloaders"]},{"cell_type":"code","metadata":{"id":"F9cQLOCkWFU5","colab_type":"code","colab":{}},"source":["class TranslationDataset(Dataset): \n","    \"\"\" \n","    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n","    Note that this class inherits torch.utils.data.Dataset\n","    return \n","    \"\"\"\n","    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n","        \"\"\" \n","        Initialize dataset by passing in a list of input indices and a list of output indices with defined maximum length for each sentence\n","        \"\"\"\n","        self.src_indices = src_indices\n","        self.targ_indices = targ_indices\n","        self.src_max_sentence_len = src_max_sentence_len\n","        self.targ_max_sentence_len = targ_max_sentence_len\n","        assert (len(self.src_indices) == len(self.targ_indices))\n","        \n","    def __len__(self): \n","        return len(self.src_indices)\n","    \n","    def __getitem__(self, key): \n","        \"\"\" \n","        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n","        respective lengths\n","        \"\"\"\n","        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n","        src_len = len(src_idx)\n","        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n","        targ_len = len(targ_idx)\n","        return [src_idx, targ_idx, src_len, targ_len]\n","    \n","\n","def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n","    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n","    \n","    src_idxs = [] \n","    targ_idxs = [] \n","    src_lens = [] \n","    targ_lens = [] \n","    \n","    for datum in batch: \n","        # append original lengths of sequences \n","        src_lens.append(datum[2]) \n","        targ_lens.append(datum[3])\n","        \n","        # pad sequences before appending \n","        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n","                                mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n","        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n","                                 mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n","        src_idxs.append(src_idx_padded)\n","        targ_idxs.append(targ_idx_padded)\n","    \n","    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n","            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n","\n","\n","def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len, batch_size): \n","    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n","        outputs a nested dictionary called 'loaders' that holds train, dev, and test loaders, \n","        e.g. loaders['dev'] holds the data loader for dev/validation set \n","    \"\"\"\n","    loaders = {} \n","    for split in ['train', 'dev', 'test']: \n","        dataset = TranslationDataset(processed_data[split]['source']['indices'], processed_data[split]['target']['indices'], \n","                                     src_max_sentence_len, targ_max_sentence_len)\n","        loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n","                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n","    return loaders"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"19-Ds_oWqH3K","colab_type":"code","colab":{}},"source":["# create dataloaders \n","## in the form \n","loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n","loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n","loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNDeUl6ipWNk","colab_type":"code","colab":{}},"source":["# examine dataloader (dont' run if you use the loders later)\n","# for src_idx, targ_idx, src_len, targ_len in loaders_full['train']:\n","#     print('source index:',src_idx,'\\ntarget index:', targ_idx, '\\nsource length', src_len, '\\ntarget length', targ_len)\n","#     break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7VbjRqUu7s7","colab_type":"text"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"vhd2ztkhu7bH","colab_type":"text"},"source":["### Encoder"]},{"cell_type":"code","metadata":{"id":"o0fZbmIPu7H7","colab_type":"code","colab":{}},"source":["# RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class EncoderRNN(nn.Module):\n","    \"\"\" RNN encoder\"\"\" \n","\n","    def __init__(self, rnn_cell_type, src_vocab_size, enc_hidden_dim, num_layers, enc_dropout, src_max_sentence_len):\n","        super(EncoderRNN, self).__init__()\n","        self.src_vocab_size = src_vocab_size\n","        self.enc_embed_dim = 300\n","        self.enc_hidden_dim = enc_hidden_dim \n","        self.enc_dropout = enc_dropout \n","        self.src_max_sentence_len = src_max_sentence_len\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding(src_vocab_size, self.enc_embed_dim)\n","        self.rnn_cell_type = rnn_cell_type \n","        if self.rnn_cell_type == 'gru': \n","            self.rnn = nn.GRU(input_size=self.enc_embed_dim, hidden_size=self.enc_hidden_dim, num_layers=self.num_layers, \n","                dropout = enc_dropout, batch_first=True, bidirectional=True) \n","        elif self.rnn_cell_type == 'lstm': \n","            self.rnn = nn.LSTM(input_size=self.enc_embed_dim, hidden_size=self.enc_hidden_dim, num_layers=self.num_layers, \n","                dropout = enc_dropout, batch_first=True, bidirectional=True)\n","\n","    def forward(self, enc_input, enc_input_lens):\n","        # save computation by packing paded sequence\n","        batch_size = enc_input.size()[0] # the number of sentences in 1 batch\n","        _, idx_sort = torch.sort(enc_input_lens, dim=0, descending=True)\n","        _, idx_unsort = torch.sort(idx_sort, dim=0)\n","        enc_input, enc_input_lens = enc_input.index_select(0, idx_sort), enc_input_lens.index_select(0, idx_sort) # 0 dimension to reselect\n","        embedded = self.embedding(enc_input) # [batch_size, seq len(the length of each sentence), emb dim(the embedding for each word in a sentence)] e.g [64,10,300]\n","        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, enc_input_lens, batch_first=True)\n","        # implement rnn\n","        hidden = self.initHidden(batch_size) \n","        if self.rnn_cell_type == 'gru': \n","            output, hidden = self.rnn(embedded, hidden)\n","        elif self.rnn_cell_type == 'lstm': \n","            memory = self.initHidden(batch_size)\n","            output, (hidden, memory) = self.rnn(embedded, (hidden, memory)) \n","        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, \n","                                                            total_length=self.src_max_sentence_len,\n","                                                            padding_value=RESERVED_TOKENS['<PAD>'])\n","        # output: (batch, seq_len, num_directions * hidden_size)\n","        # get the output and hidden in the original unsorted order\n","        output = output.index_select(0, idx_unsort)\n","        hidden = hidden.index_select(1, idx_unsort)\n","        output = torch.cat([output[:, :, :self.enc_hidden_dim], output[:, :, self.enc_hidden_dim:]], dim=2)\n","        hidden = hidden.view(self.num_layers, 2, batch_size, self.enc_hidden_dim) # # h_n.view(num_layers, num_directions, batch, hidden_size)\n","        hidden = torch.cat([hidden[:, 0, :, :].view(self.num_layers, 1, batch_size, self.enc_hidden_dim).squeeze(dim=1), \n","            hidden[:, 1, :, :].view(self.num_layers, 1, batch_size, self.enc_hidden_dim).squeeze(dim=1)], dim=2) \n","        hidden = hidden.view(self.num_layers, batch_size, 2 * self.enc_hidden_dim)\n","\n","        return output, hidden\n","\n","    def initHidden(self, batch_size):\n","        return torch.zeros(2*self.num_layers, batch_size, self.enc_hidden_dim).to(device)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIJdgWplu6--","colab_type":"text"},"source":["### Decoder"]},{"cell_type":"code","metadata":{"id":"F06VQX-Ku62S","colab_type":"code","colab":{}},"source":["class DecoderRNN(nn.Module): \n","\n","\t\"\"\" Vanilla decoder without attention, but final layer from encoder is repeatedly passed as input to each time step. \n","\t\tHandles output from EncoderRNN, which concats bidirectional output. \n","\t\"\"\" \n","\n","\tdef __init__(self, dec_hidden_dim, enc_hidden_dim, num_layers, targ_vocab_size, targ_max_sentence_len):\n","\t\tsuper(DecoderRNN, self).__init__()\n","\t\tself.dec_embed_dim = 300\n","\t\tself.dec_hidden_dim = dec_hidden_dim \n","\t\tself.enc_hidden_dim = enc_hidden_dim\n","\t\tself.targ_vocab_size = targ_vocab_size\n","\t\tself.targ_max_sentence_len = targ_max_sentence_len\n","\t\tself.num_layers = num_layers\n","\t\tself.embedding = nn.Embedding(targ_vocab_size, self.dec_embed_dim)\n","\t\tself.gru = nn.GRU(self.dec_embed_dim + 2 * self.enc_hidden_dim, self.dec_hidden_dim, num_layers=self.num_layers) \n","\t\tself.out = nn.Linear(dec_hidden_dim, self.targ_vocab_size) \n","\t\tself.softmax = nn.LogSoftmax(dim=1) \n","\n","\tdef forward(self, dec_input, dec_hidden, enc_outputs): \n","\t\tdec_input = dec_input \n","\t\tdec_hidden = dec_hidden \n","\t\tenc_outputs = enc_outputs \n","\t\tbatch_size = dec_input.size()[0]\n","\t\tembedded = self.embedding(dec_input).view(1, batch_size, -1)\t\n","#\t\tcontext = enc_outputs[:, -1, :].unsqueeze(dim=1).transpose(0, 1) \n","\t\tcontext = torch.cat([enc_outputs[:, -1, :self.enc_hidden_dim], \n","\t\t\t\t\t\t\t enc_outputs[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0)\n","\t\tconcat = torch.cat([embedded, context], 2) \n","\t\toutput, hidden = self.gru(concat, dec_hidden)\n","\t\toutput = self.softmax(self.out(output[0]))  \n","\t\treturn output, hidden\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eKcvBFUau6tj","colab_type":"text"},"source":["### Bidirectional Encoder-Decoder"]},{"cell_type":"code","metadata":{"id":"OW5q5aTQu6dY","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module): \n","\n","\t\"\"\" Encoder-Decoder without attention \"\"\"\n","\n","\tdef __init__(self, encoder, decoder, decoder_token2id): \n","\t\tsuper(EncoderDecoder, self).__init__() \n","\t\tself.encoder = encoder \n","\t\tself.decoder = decoder \n","\t\tself.targ_vocab_size = self.decoder.targ_vocab_size\n","\t\tself.src_max_sentence_len = self.encoder.src_max_sentence_len \n","\t\tself.targ_max_sentence_len = self.decoder.targ_max_sentence_len \n","\n","\tdef forward(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio): \n","\t\t\n","\t\tbatch_size = src_idx.size()[0]\n","\t\tenc_outputs, enc_hidden = self.encoder(src_idx, src_lens)\n","\t\tdec_hidden = enc_hidden \n","\t\tdec_outputs = Variable(torch.zeros(self.targ_max_sentence_len, batch_size, self.targ_vocab_size))\n","\t\thypotheses = Variable(torch.zeros(self.targ_max_sentence_len, batch_size))\n","\t\tdec_output = targ_idx[:, 0] \n","\n","\t\tfor di in range(1, self.targ_max_sentence_len): \n","\t\t\tdec_output, dec_hidden = self.decoder(dec_output, dec_hidden, enc_outputs)\n","\t\t\tdec_outputs[di] = dec_output \n","\t\t\tteacher_labels = targ_idx[:, di-1] \n","\t\t\tgreedy_labels = dec_output.data.max(1)[1]\n","\t\t\tdec_output = teacher_labels if random.random() < teacher_forcing_ratio else greedy_labels \n","\t\t\thypotheses[di] = greedy_labels\n","\n","\t\tattn_placeholder = Variable(torch.zeros(batch_size, self.targ_max_sentence_len, self.src_max_sentence_len))\n","\n","\t\treturn dec_outputs, hypotheses.transpose(0,1), attn_placeholder \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P2-GF1kWu6WI","colab_type":"text"},"source":["## Train and Evaluate"]},{"cell_type":"code","metadata":{"id":"aTU2Y4HBu6P0","colab_type":"code","colab":{}},"source":["#RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n","RESULTS_LOG = '/content/drive/My Drive/ds1012/MT/experiment_results/experiment_results_log.pkl'\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def filter_reserved_tokens(sentence_as_list): \n","    \"\"\" Takes a list of tokens representing a sentence, removes everything after <EOS>, \n","    as well as remove reserved tokens <SOS>, <EOS>, <PAD>. Outputs filtered sentence as a string. \"\"\" \n","\n","    # drops everything after <EOS> \n","    try: \n","        output = sentence_as_list[:sentence_as_list.index('<EOS>')]\n","    except: \n","        output = sentence_as_list\n","\n","    # drops <SOS>, <EOS>, <PAD>  \n","    output = ' '.join([idx for idx in output if idx not in ['<SOS>', '<EOS>', '<PAD>']]) \n","\n","    return output \n","\n","\n","def tensor2corpus(tensor, id2token):  \n","    \"\"\" Takes a tensor representing a batch of sentences (size: batch_size * max_sentence_length), and returns \n","        its token equivalent (as list of tokens) \"\"\" \n","    \n","    list_of_lists = tensor.cpu().numpy().astype(int).tolist()\n","    to_token = lambda l: [id2token[idx] for idx in l]\n","    corpus = [to_token(l) for l in list_of_lists] \n"," \n","    return corpus\n","\n","\n","def reconstruct_corpus(token_list): \n","    \"\"\" Takes a list of tokens, filter out reserved tokens, and returns a list of sentence strings \"\"\" \n","\n","    sentences = [filter_reserved_tokens(sublist) for sublist in token_list]\n","\n","    return sentences  \n","\n","\n","def calc_corpus_bleu(ref_list, hyp_list): \n","    \"\"\" Takes a list of reference sentences and a list of hypothesis sentences, flattens them, and outputs their corpus bleu \"\"\"\n","\n","    # convert ref_list and hyp_list into strings \n","    hyp_stream = reconstruct_corpus(hyp_list)\n","    ref_streams = [reconstruct_corpus(ref_list)]\n","    \n","    # compute bleu score \n","    bleu_score = sacrebleu.corpus_bleu(hyp_stream, ref_streams).score  \n","\n","    return bleu_score \n","\n","\n","def evaluate(model, loader, src_id2token, targ_id2token, teacher_forcing_ratio=1): \n","    \"\"\" Evaluates a model given a loader, id2token dicts, and teacher_forcing_ratio. \n","        Outputs avg loss, avg bleu, as well as indices and tokens representing source, reference, and model translations. \n","    \"\"\"\n","    \n","    with torch.no_grad():\n","\n","        model.eval() \n","        total_loss = 0 \n","\n","        # initialize empty list to hold all source, reference and model translations \n","        reference_corpus = []\n","        hypothesis_corpus = [] \n","        source_corpus = [] \n","        attn_weights_corpus = []\n","        \n","        for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader): \n","\n","            # for each batch, compute loss and accumulate to total \n","            batch_size = src_idxs.size()[0]        \n","            src_idxs, targ_idxs, src_lens, targ_lens = src_idxs.to(device), targ_idxs.to(device), src_lens.to(device), targ_lens.to(device)\n","            outputs, hypotheses, attn_weights = model(src_idxs, targ_idxs, src_lens, targ_lens, \n","                teacher_forcing_ratio=teacher_forcing_ratio)\n","            outputs = outputs[1:].transpose(0, 1)\n","            targets = targ_idxs[:,1:]\n","            attn_weights = attn_weights[:,1:]\n","            outputs_for_nll = outputs.contiguous().view(-1, model.decoder.targ_vocab_size).to(device)\n","            targets_for_nll = targets.contiguous().view(-1).to(device)\n","            loss = F.nll_loss(outputs_for_nll, targets_for_nll, ignore_index=RESERVED_TOKENS['<PAD>'])        \n","            total_loss += loss.item()  \n","\n","            # append to lists holding corpus \n","            hypothesis_corpus.append(hypotheses)\n","            reference_corpus.append(targets)\n","            source_corpus.append(src_idxs)\n","            attn_weights_corpus.append(attn_weights)\n","\n","    # concat list of index tensors into corpus tensors (as indices), then convert to list of sentence (as tokens)\n","    hyp_idxs = torch.cat(hypothesis_corpus, dim=0) \n","    ref_idxs = torch.cat(reference_corpus, dim=0)\n","    source_idxs = torch.cat(source_corpus, dim=0)\n","    attn = torch.cat(attn_weights_corpus, dim=0)\n","\n","    hyp_tokens = tensor2corpus(hyp_idxs, targ_id2token)\n","    ref_tokens = tensor2corpus(ref_idxs, targ_id2token)\n","    source_tokens = tensor2corpus(source_idxs, src_id2token)\n","\n","    # compute evaluation metrics \n","    avg_loss = total_loss / len(loader)\n","    avg_bleu = calc_corpus_bleu(ref_tokens, hyp_tokens)\n","    \n","    return avg_loss, avg_bleu, hyp_idxs, ref_idxs, source_idxs, hyp_tokens, ref_tokens, source_tokens, attn   \n","\n","\n","def train_and_eval(model, loaders_full, loaders_minibatch, loaders_minitrain, params, vocab, \n","    lazy_eval=True, print_intermediate=1000000, save_checkpoint=True, save_to_log=True, inspect_samples=None, print_attn=False): \n","    \n","    \"\"\" Main function to train and evaluate model: takes a model, loaders, and a bunch of parameters and \n","        returns trained model along with a results dict storing epoch, train/val loss, and train/val bleu scores. \n","        Note that: \n","        - lazy_train = train and validate only on a single mini batch (for quick prototyping) \n","        - lazy_eval = skip evaluation on train set altogether (not even the 1K proxy) \n","        - print_intermediate = reports loss and bleu scores every 'print_intermediate' minibatches or end of each epoch \n","        - save_checkpoint = saves model's state dict into a .pth.tar file named after model_name \n","        - save_to_log = saves results to log \n","        - inspect_samples = specify number of samples to print out every 1K batches \n","    \"\"\"\n","    \n","    start_time = time.time() \n","\n","    # extract local variables from params \n","    learning_rate = params['learning_rate'] \n","    targ_id2token = vocab[params['targ_lang']]['id2token']\n","    src_id2token = vocab[params['src_lang']]['id2token']\n","    num_epochs = params['num_epochs']\n","    teacher_forcing_ratio = params['teacher_forcing_ratio']\n","    clip_grad_max_norm = params['clip_grad_max_norm']\n","    experiment_name = params['experiment_name']\n","    model_name = params['model_name']\n","    lazy_train = params['lazy_train']\n","    attention_type = params['attention_type']\n","\n","    # designate data loaders used to train and calculate losses \n","    if lazy_train: \n","        train_loader_ = loaders_minibatch['train'] # used to train \n","        dev_loader_ = loaders_minibatch['dev'] # used to calculate dev loss \n","        train_loader_proxy = loaders_minibatch['train'] # used to calculate train loss \n","    else: \n","        train_loader_ = loaders_full['train']\n","        dev_loader_ = loaders_full['dev'] \n","        # evaluating on full training set prohibitively expensive, so use a 1K batch instead as proxy \n","        train_loader_proxy = loaders_minitrain['train'] \n","\n","    # initialize optimizer and criterion \n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.NLLLoss(ignore_index=RESERVED_TOKENS['<PAD>'])\n","    results = [] \n","    \n","    # loop through train data in batches and train \n","    for epoch in range(num_epochs): \n","        train_loss = 0 \n","        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader_):\n","            DEBUG_START = time.time() \n","            src_idxs, targ_idxs, src_lens, targ_lens = src_idxs.to(device), targ_idxs.to(device), src_lens.to(device), targ_lens.to(device)\n","            model.train()\n","            optimizer.zero_grad()\n","            final_outputs, hypotheses, attn_weights = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=teacher_forcing_ratio) \n","            # attn_weights = attn_weights[:,1:]\n","            final_outputs = final_outputs[1:].transpose(0, 1)\n","            targets = targ_idxs[:,1:]\n","            outputs_for_nll = final_outputs.contiguous().view(-1, model.decoder.targ_vocab_size).to(device)\n","            targets_for_nll = targets.contiguous().view(-1).to(device)\n","            loss = criterion(outputs_for_nll, targets_for_nll)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_max_norm)\n","            optimizer.step()\n","            \n","            # evaluate and report loss and bleu scores every 'print_intermediate' minibatches or end of each epoch\n","            if batch % print_intermediate == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader_)-1)):\n","\n","                result = {} \n","                result['epoch'] = epoch + batch / len(train_loader_) \n","\n","                # calculate metrics on validation set \n","                result['val_loss'], result['val_bleu'], val_hyp_idxs, val_ref_idxs, val_source_idxs, val_hyp_tokens, val_ref_tokens, val_source_tokens, val_attn = \\\n","                    evaluate(model, dev_loader_, src_id2token, targ_id2token, teacher_forcing_ratio=teacher_forcing_ratio)         \n","\n","                # calculate metrics on train set (or proxy thereof) only if lazy_eval not set to True \n","                if not lazy_eval: \n","                    result['train_loss'], result['train_bleu'], train_hyp_idxs, train_ref_idxs, train_source_idxs, train_hyp_tokens, train_ref_tokens, train_source_tokens, train_attn = \\\n","                            evaluate(model, train_loader_proxy, src_id2token, targ_id2token, teacher_forcing_ratio=teacher_forcing_ratio) \n","                else: \n","                    result['train_loss'], result['train_bleu'] = 0, 0  \n","\n","                results.append(result)\n","\n","                print('Epoch: {:.2f}, Train Loss: {:.2f}, Val Loss: {:.2f}, Train BLEU: {:.2f}, Val BLEU: {:.2f}, Minutes Elapsed: {:.2f}'\\\n","                      .format(result['epoch'], result['train_loss'], result['val_loss'], \n","                              result['train_bleu'], result['val_bleu'], (time.time() - start_time) / 60 ))\n","                    \n","                if inspect_samples is not None: \n","                    # sample predictions from training set, if available \n","                    if not lazy_eval: \n","                        print(\"Sampling from training predictions...\")\n","                        sample_predictions(train_hyp_idxs, train_ref_idxs, train_source_idxs, train_hyp_tokens, train_ref_tokens, \n","                            train_source_tokens, targ_id2token, train_attn, print_attn=print_attn, num_samples=inspect_samples)\n","                    # sample predictions from validation set \n","                    print(\"Sampling from val predictions...\")\n","                    sample_predictions(val_hyp_idxs, val_ref_idxs, val_source_idxs, val_hyp_tokens, val_ref_tokens, val_source_tokens, \n","                        targ_id2token, val_attn, print_attn=print_attn, num_samples=inspect_samples)\n","                    \n","                if save_checkpoint: \n","                    if result['val_bleu'] == pd.DataFrame.from_dict(results)['val_bleu'].max(): \n","                        checkpoint_fp = '/content/drive/My Drive/ds1012/MT/model_checkpoints/{}.pth.tar'.format(model_name)\n","                        check_dir_exists(filename=checkpoint_fp)\n","                        torch.save(model.state_dict(), checkpoint_fp)\n"," \n","    runtime = (time.time() - start_time) / 60 \n","    dt_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    total_params, trainable_params = count_parameters(model)               \n","\n","    if save_to_log: \n","        append_to_log(params, results, runtime, experiment_name, model_name, dt_created, total_params, trainable_params)\n","\n","    print(\"Model training completed in {} minutes with {:.2f} best validation loss and {:.2f} best validation BLEU.\".format(\n","        int(runtime), pd.DataFrame.from_dict(results)['val_loss'].min(), \n","        pd.DataFrame.from_dict(results)['val_bleu'].max()))\n","\n","    return model, results  \n","\n","def sample_predictions(hyp_idxs, ref_idxs, source_idxs, hyp_tokens, ref_tokens, source_tokens, id2token, \n","    attn, print_attn, num_samples=1, ): \n","\n","    \"\"\" Sample a few source sentences, reference and model translations to review \"\"\" \n","\n","    for i in range(num_samples): \n","        rand = random.randint(0, len(hyp_idxs)-1) \n","        source = ' '.join(source_tokens[rand])\n","        print(\"Source: {}\".format(source))\n","        reference_translation = ' '.join(ref_tokens[rand]) \n","        print(\"Reference: {}\".format(reference_translation))\n","        model_translation = ' '.join(hyp_tokens[rand])\n","        print(\"Model: {}\".format(model_translation))\n","        if print_attn: \n","            print(\"Attention Weights: {}\".format(attn[rand]))\n","        print()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hq-dDt3lc9Wi","colab_type":"code","colab":{}},"source":["def check_dir_exists(filename): \n","    \"\"\" Takes filename string and check whether its implied directory exists, otherwise creates it \"\"\" \n","\n","    if not os.path.exists(os.path.dirname(filename)):\n","        os.makedirs(os.path.dirname(filename))\n","    else: \n","        pass \n","        \n","\n","def append_to_log(hyperparams, results, runtime, experiment_name, model_name, dt_created, total_params, trainable_params, filename=RESULTS_LOG): \n","    \"\"\" Appends results and details of a single experiment to a log file \"\"\"\n","    \n","    # check directory exists, else creates it \n","    check_dir_exists(filename)\n","        \n","    # store experiment details in a dictionary \n","    new_result = {'experiment_name': experiment_name, 'model_name': model_name, 'hyperparams': hyperparams, \n","        'results': results, 'runtime': runtime, 'dt_created': dt_created, \n","        'total_params': total_params, 'trainable_params': trainable_params}\n","    \n","    # if log already exists, append to log \n","    try: \n","        results_log = pkl.load(open(filename, \"rb\"))\n","        results_log.append(new_result)\n","\n","    # if log doesn't exists, initialize first result as the log \n","    except (OSError, IOError) as e:\n","        results_log = [new_result]\n","    \n","    # save to pickle \n","    pkl.dump(results_log, open(filename, \"wb\")) \n","\n","\n","def load_experiment_log(experiment_name=None, model_name=None, filename=RESULTS_LOG): \n","    \"\"\" Loads experiment log, with option to filter for a specific experiment_name \"\"\"\n","    \n","    results_log = pkl.load(open(filename, \"rb\"))\n","    \n","    if experiment_name is not None: \n","        results_log = [r for r in results_log if r['experiment_name'] == experiment_name]\n","\n","    if model_name is not None: \n","        results_log = [r for r in results_log if r['model_name'] == model_name]\n","\n","    # sort by dt_created \n","    results_log = sorted(results_log, key=lambda k: k['dt_created'], reverse=True)\n","        \n","    return results_log\n","\n","\n","def summarize_results(results_log): \n","    \"\"\" Summarizes results_log (list) into a dataframe, splitting hyperparameters string into columns, and reducing \n","        the val_acc dict into the best validation accuracy obtained amongst all the epochs logged \"\"\"\n","    results_df = pd.DataFrame.from_dict(results_log)\n","    results_df = pd.concat([results_df, results_df['hyperparams'].apply(pd.Series)], axis=1)\n","    results_df = results_df.loc[:, ~results_df.columns.duplicated()] # unfortunately saved model_name and experiment_name twice \n","    results_df['best_val_loss'] = results_df['results'].apply(lambda d: pd.DataFrame.from_dict(d)['val_loss'].min())\n","    results_df['best_val_bleu'] = results_df['results'].apply(lambda d: pd.DataFrame.from_dict(d)['val_bleu'].max())\n","    return results_df.sort_values(by='dt_created', ascending=False) \n","\n","\n","def count_parameters(model): \n","    \"\"\" Returns total and trainable parameters of a given model \"\"\" \n","    all_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    return all_params, trainable_params"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hv61C_BjRe37","colab_type":"code","colab":{}},"source":["# model architecture params \n","NETWORK_TYPE = 'rnn'\n","RNN_CELL_TYPE = 'gru'\n","NUM_LAYERS = 2 \n","ENC_HIDDEN_DIM = 512\n","DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n","TEACHER_FORCING_RATIO = 1\n","CLIP_GRAD_MAX_NORM = 1\n","ENC_DROPOUT = 0 \n","DEC_DROPOUT = 0  \n","ATTENTION_TYPE = 'without'\n","\n","# training params  \n","NUM_EPOCHS = 10 \n","LR = 0.00015 \n","OPTIMIZER = 'Adam'\n","LAZY_TRAIN = False\n","\n","# name the model and experiment \n","if NETWORK_TYPE == 'rnn': \n","    EXPERIMENT_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n","elif NETWORK_TYPE == 'cnn': \n","    EXPERIMENT_NAME = '{}-cnn'.format(SRC_LANG)\n","MODEL_NAME = '{}-{}'.format(EXPERIMENT_NAME, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1nv-1_ZgsDZ","colab_type":"code","colab":{}},"source":["# store as dict to save to results later \n","params = {'experiment_name': EXPERIMENT_NAME,'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n","          'rnn_cell_type': RNN_CELL_TYPE, 'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, \n","          'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, 'src_vocab_size': SRC_VOCAB_SIZE, \n","          'targ_vocab_size': TARG_VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, \n","          'dec_hidden_dim': DEC_HIDDEN_DIM, 'teacher_forcing_ratio': TEACHER_FORCING_RATIO, \n","          'clip_grad_max_norm': CLIP_GRAD_MAX_NORM, 'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, \n","          'attention_type': ATTENTION_TYPE, 'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, \n","          'learning_rate': LR, 'optimizer': OPTIMIZER, 'lazy_train': LAZY_TRAIN}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9X5tqYQmjp_8","colab_type":"code","colab":{}},"source":["encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, src_vocab_size = SRC_VOCAB_SIZE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n","                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT)\n","\n","# without attention \n","decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n","                        targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN)\n","\n","model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kq4PLDErkTGz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"bb995f8c-86da-4b34-ba53-30eaef34ae7e","executionInfo":{"status":"ok","timestamp":1587270262108,"user_tz":240,"elapsed":573575,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["model, results = train_and_eval(\n","    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n","    params=params, vocab=vocab, print_intermediate=500, save_checkpoint=True, save_to_log=True, \n","    lazy_eval=True, print_attn=False, inspect_samples=3)"],"execution_count":359,"outputs":[{"output_type":"stream","text":["Epoch: 0.00, Train Loss: 0.00, Val Loss: 9.08, Train BLEU: 0.00, Val BLEU: 0.01, Minutes Elapsed: 0.06\n","Sampling from val predictions...\n","Source: <UNK> and <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: <UNK> او <UNK> unicode block name <EOS> <PAD> <PAD>\n","Model: <SOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Source: \" <UNK> you would not do that ! Would you\n","Reference: په رښتيا سره تۀ به د بد <UNK> سره\n","Model: <SOS> UTF ګډون ذريعه دوى ولېږل کوډې ښول کوډې اېنټرنېټ\n","\n","Source: Thu al - Hijjah <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: دوه نۍ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Model: <SOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Epoch: 1.00, Train Loss: 0.00, Val Loss: 4.85, Train BLEU: 0.00, Val BLEU: 8.41, Minutes Elapsed: 1.02\n","Sampling from val predictions...\n","Source: Selection <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: ټاکنه <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Model: <SOS> <UNK> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Source: \"‘ You may also buy the children of the foreigners\n","Reference: تاسو د هغه م ُ سافرو اولاد هم اخستلے\n","Model: <SOS> د د د د ُ ونږ د ُ د\n","\n","Source: Font size for label <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: د نښکې د ليکبڼې کچ <EOS> <PAD> <PAD> <PAD>\n","Model: <SOS> د - action <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Epoch: 2.00, Train Loss: 0.00, Val Loss: 4.42, Train BLEU: 0.00, Val BLEU: 11.14, Minutes Elapsed: 1.96\n","Sampling from val predictions...\n","Source: Disk view columns order <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: د ټيکلي ليد ستنو <UNK> <EOS> <PAD> <PAD> <PAD>\n","Model: <SOS> د <UNK> نه نه <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Source: Text Only <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: يوازې ليکنه <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Model: <SOS> د <UNK> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Source: <UNK> / Postal Code : <EOS> <PAD> <PAD> <PAD> <PAD>\n","Reference: : <UNK> / <UNK> <UNK> <EOS> <PAD> <PAD> <PAD>\n","Model: <SOS> : <UNK> <UNK> <UNK> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Epoch: 3.00, Train Loss: 0.00, Val Loss: 4.20, Train BLEU: 0.00, Val BLEU: 12.28, Minutes Elapsed: 2.91\n","Sampling from val predictions...\n","Source: ( Laughter ) <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: ( په خندا ) <EOS> <PAD> <PAD> <PAD> <PAD>\n","Model: <SOS> ( ( ) ) <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Source: But this man does not have their genealogy . He\n","Reference: خو اګر که د َ ملک ِ ص ِ\n","Model: <SOS> خو چه چه چه د دے ُ َ په\n","\n","Source: He said : \" Take heed that you are not\n","Reference: عيسى ٰ ووئيل ، ” پام کوئ چه څوک\n","Model: <SOS> هغۀ جواب جواب ته ” تاسو تاسو چه تاسو\n","\n","Epoch: 4.00, Train Loss: 0.00, Val Loss: 4.06, Train BLEU: 0.00, Val BLEU: 12.81, Minutes Elapsed: 3.85\n","Sampling from val predictions...\n","Source: Their <UNK> ran off to the town to tell others\n","Reference: د َ کنډک شپ ُ ونکى په تيښته لاړل\n","Model: <SOS> نو َ نه د َ َ ته ُ کښے\n","\n","Source: <UNK> <UNK> ( instead of checking ) <EOS> <PAD> <PAD>\n","Reference: <UNK> بيا جوړول ( د کتلو پر ځای )\n","Model: <SOS> <UNK> - <UNK> <UNK> <EOS> <EOS> <UNK> <EOS> <EOS>\n","\n","Source: the altar on which to burn offerings , with its\n","Reference: ق ُ ربان ‌ ګاه چې په ک ُ\n","Model: <SOS> د دې چې د چې چې چې چې کښې\n","\n","Epoch: 5.00, Train Loss: 0.00, Val Loss: 3.96, Train BLEU: 0.00, Val BLEU: 13.20, Minutes Elapsed: 4.80\n","Sampling from val predictions...\n","Source: \" You know that we brought back to you from\n","Reference: تاسو ته پته ده چې م ُ ونږ د\n","Model: <SOS> تاسو تاسو د چې چې تاسو ُ ونږ ته\n","\n","Source: until we all <UNK> to the unity of the faith\n","Reference: تر څو چه م ُ ونږ ټول په ايمان\n","Model: <SOS> نو َ چه چه ُ ونږ د َ ُ\n","\n","Source: Full Volume <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: ټول <UNK> %% d \" to \"% Id %\n","Model: <SOS> د <UNK> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Epoch: 6.00, Train Loss: 0.00, Val Loss: 3.90, Train BLEU: 0.00, Val BLEU: 13.91, Minutes Elapsed: 5.75\n","Sampling from val predictions...\n","Source: <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: د <UNK> غږيز <UNK> <UNK> unicode block name <EOS>\n","Model: <SOS> <UNK> <UNK> <UNK> <EOS> <EOS> <EOS> <EOS> name <EOS>\n","\n","Source: Balak did as he was told . He and Balaam\n","Reference: بلق هم هغه شان وکړل چې څنګه بلعام وئيلى\n","Model: <SOS> نو چې هغه شان چې چې چې هغوئ ِ\n","\n","Source: \" Again he sent another servant and they treated him\n","Reference: بيا هغۀ يو بل خدمتګار ورواستولو ا َ ؤ\n","Model: <SOS> هغۀ َ ٰ َ ُ َ ا َ ؤ\n","\n","Epoch: 7.00, Train Loss: 0.00, Val Loss: 3.86, Train BLEU: 0.00, Val BLEU: 14.43, Minutes Elapsed: 6.71\n","Sampling from val predictions...\n","Source: <UNK> delete file \"% s \"? <EOS> <PAD> <PAD> <PAD>\n","Reference: دوتنه ړنګول غواړﺉ ؟ \"% s \" په رښتيا\n","Model: <SOS> <UNK> s s s s s \" <EOS> \"\n","\n","Source: \" Do not be like them . Your Father knows\n","Reference: د َ هغو <UNK> مه کوئ ستاسو آسمانى پلار\n","Model: <SOS> د د نه نه َ َ ، د َ\n","\n","Source: Set font size <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: د ليکبڼې کچ ټاکل <EOS> <PAD> <PAD> <PAD> <PAD>\n","Model: <SOS> د - <UNK> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Epoch: 8.00, Train Loss: 0.00, Val Loss: 3.85, Train BLEU: 0.00, Val BLEU: 14.28, Minutes Elapsed: 7.66\n","Sampling from val predictions...\n","Source: There is no respect reverence for God before their eyes\n","Reference: ا َ ؤ د َ خ ُ دائے ويره\n","Model: <SOS> ځکه د ؤ د د خ ُ دائے په\n","\n","Source: Save Search <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: پلټون <UNK> : <UNK> <UNK> <UNK> \", or \"\n","Model: <SOS> ساتل ساتل <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Source: \" I spoke to you then and said : ‘\n","Reference: م ُ وسى ٰ خلقو ته ووئيل ، ”\n","Model: <SOS> ما َ ونږ ٰ ته ته ا ، ”\n","\n","Epoch: 9.00, Train Loss: 0.00, Val Loss: 3.84, Train BLEU: 0.00, Val BLEU: 14.52, Minutes Elapsed: 8.60\n","Sampling from val predictions...\n","Source: Redo the last operation that was undone , <UNK> visibility\n","Reference: edit - action <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Model: <SOS> layers - action <EOS> action <EOS> <EOS> <UNK> <UNK>\n","\n","Source: Stock label <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: <UNK> label <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Model: <SOS> Stock label <EOS> name <EOS> <EOS> <EOS> <EOS> <EOS>\n","\n","Source: Stock label <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Reference: چاپ <UNK> label <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Model: <SOS> Stock label <EOS> label media <EOS> <EOS> <EOS> <EOS>\n","\n","Epoch: 10.00, Train Loss: 0.00, Val Loss: 3.87, Train BLEU: 0.00, Val BLEU: 14.81, Minutes Elapsed: 9.55\n","Sampling from val predictions...\n","Source: God delivered us out of so great a death ,\n","Reference: نو هم هغۀ م ُ ونږ د َ داسے\n","Model: <SOS> خ ُ ُ ُ ُ ونږ ٰ د نه\n","\n","Source: I ' m not here right now <EOS> <PAD> <PAD>\n","Reference: زه اوس دلته نه يم <EOS> <PAD> <PAD> <PAD>\n","Model: <SOS> زه نه نه نه نه لري لري <EOS> <EOS>\n","\n","Source: He sent two of his helpers Timothy and <UNK> to\n","Reference: نو هغۀ دوه ملګرى تيموتيوس ا َ ؤ ا\n","Model: <SOS> هغۀ هغۀ د ُ ِ ُ اؤ ؤ د\n","\n","Model training completed in 9 minutes with 3.84 best validation loss and 14.81 best validation BLEU.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_CHMB1Nbngrz","colab_type":"text"},"source":["## Show dev and test results"]},{"cell_type":"code","metadata":{"id":"DA1Gcfl1rqZ4","colab_type":"code","colab":{}},"source":["def plot_single_learning_curve(results, figsize=(14, 5)): \n","    \"\"\" Plots learning curve of a SINGLE experiment \"\"\"\n","    results_df = pd.DataFrame.from_dict(results)\n","    results_df = results_df.set_index('epoch')\n","    results_df = results_df[['val_bleu', 'val_loss']]\n","\n","    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n","    results_df['val_loss'].plot(ax=axes[0])\n","    axes[0].set_ylabel('Validation Loss')\n","    results_df['val_bleu'].plot(ax=axes[1])\n","    axes[1].set_ylabel('Validation BLEU')\n","    axes[0].set_xlabel('Epoch')\n","    axes[1].set_xlabel('Epoch')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQ8ttWthk5dx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":334},"outputId":"79dbc32a-113b-42d8-bc9c-339202888166","executionInfo":{"status":"ok","timestamp":1587270300838,"user_tz":240,"elapsed":600,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["experiment_results = load_experiment_log(experiment_name=EXPERIMENT_NAME)\n","plot_single_learning_curve(experiment_results[0]['results'])"],"execution_count":362,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1008x360 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"RGA9kMUZmhEr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"889a04d8-7cb4-4745-9de2-f119870f621b","executionInfo":{"status":"ok","timestamp":1587270302699,"user_tz":240,"elapsed":314,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["summarize_results(experiment_results)[['model_name', 'best_val_loss', 'best_val_bleu', 'runtime', \n","                                       'total_params', 'trainable_params', 'dt_created']]"],"execution_count":363,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model_name</th>\n","      <th>best_val_loss</th>\n","      <th>best_val_bleu</th>\n","      <th>runtime</th>\n","      <th>total_params</th>\n","      <th>trainable_params</th>\n","      <th>dt_created</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>en-rnn-without-attn-2020-04-19 04:14:47</td>\n","      <td>3.840112</td>\n","      <td>14.806068</td>\n","      <td>9.553993</td>\n","      <td>36992144</td>\n","      <td>36992144</td>\n","      <td>2020-04-19 04:24:21</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                model_name  ...           dt_created\n","0  en-rnn-without-attn-2020-04-19 04:14:47  ...  2020-04-19 04:24:21\n","\n","[1 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":363}]},{"cell_type":"code","metadata":{"id":"h_LGqZE-nuXv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6f45c414-fc3c-42c4-edd4-c683271cafff","executionInfo":{"status":"ok","timestamp":1587271017317,"user_tz":240,"elapsed":523,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["# reload model \n","MODEL_NAME_TO_RELOAD = 'en-rnn-without-attn-2020-04-19 04:14:47'\n","checkpoint = torch.load('/content/drive/My Drive/ds1012/MT/model_checkpoints/{}.pth.tar'.format(MODEL_NAME_TO_RELOAD), map_location=device)\n","model.load_state_dict(checkpoint)"],"execution_count":366,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":366}]},{"cell_type":"code","metadata":{"id":"iGtDelPvnvYv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"313cabe7-5ac3-4e7b-9917-438e4461b861","executionInfo":{"status":"ok","timestamp":1587271026013,"user_tz":240,"elapsed":4327,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["# check performance on validation set \n","val_loss, val_bleu, val_hyp_idxs, val_ref_idxs, val_source_idxs, val_hyp_tokens, val_ref_tokens, val_source_tokens,\\\n","val_attn = evaluate(model=model, loader=loaders_full['dev'], \n","                    src_id2token=vocab[SRC_LANG]['id2token'], targ_id2token=vocab[TARG_LANG]['id2token'])\n","print(\"Validation BLEU: {:.2f} | Validation Loss: {:.2f}\".format(val_bleu, val_loss))"],"execution_count":367,"outputs":[{"output_type":"stream","text":["Validation BLEU: 14.81 | Validation Loss: 3.87\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EmgtuBRSnxz6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4a87b972-677b-447c-cf31-e4367ed6cb3a","executionInfo":{"status":"ok","timestamp":1587271031237,"user_tz":240,"elapsed":4050,"user":{"displayName":"Zihao Guo","photoUrl":"","userId":"12502228106172909245"}}},"source":["test_loss, test_bleu, test_hyp_idxs, test_ref_idxs, test_source_idxs, test_hyp_tokens, test_ref_tokens, test_source_tokens,\\\n","test_attn = evaluate(model=model, loader=loaders_full['test'], \n","                     src_id2token=vocab[SRC_LANG]['id2token'], targ_id2token=vocab[TARG_LANG]['id2token'])\n","print(\"Test BLEU: {:.2f} | Test Loss: {:.2f}\".format(test_bleu, test_loss))"],"execution_count":368,"outputs":[{"output_type":"stream","text":["Test BLEU: 14.53 | Test Loss: 3.89\n"],"name":"stdout"}]}]}